# 关于频域学习

之前在做某一个项目时，想把模型的计算量降低、参数量减少，还想做得“花哨”一点。加之现在CV方向的深度学习相关的研究大多都是基于RGB色域（或者说是空间域、空域）进行的，所以突发奇想能不能把这个空域变换到频域上去呢？毕竟从人眼的角度出发，人眼总是对低频的信息更为敏感，而对高频的信息（比如肉眼不可见的噪声）不是那么敏感。然后我去找了一下频域学习在目标检测、图像分割这些方面的应用，发现目前发表的工作不是很多。以下是我对频域学习现有工作的一些总结和思考。（截止2021年3月23日）



## 一、引出问题

* 一般情况下，大家对CNN卷积神经网络提取特征的方式的共识都是：最开始提取**low-level**的特征，例如边缘轮廓、纹理等等，然后随着卷积层的加深，网络开始提取**high-level**的抽象特征。实际上这一过程，可以说是CNN网络在隐式对频域进行建模。结合随着CNN的层数逐渐加深，深层特征层上某一点的特征对应的感受野也不断扩大，CNN的这些行为和人类观察事物的过程相当类似。**但是有时候，CNN有时候表现出的行为却让人难以直观解释**。例如下图中，输入一张普通“熊猫”的图片，其预测置信度为57.7%，如果在这张熊猫图片上添加一个**不可察觉的噪声层**，会导致卷积神经网络将熊猫误识别为长臂猿，而且置信度高达99.3%。

  <div align="center">
  <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210305172739085.png" width=700>   
  </div>

* 这些情况通常被称为“对抗样本”（adversarial examples），不经意地就能改变AI模型的行为。对抗机器学习是当前人工智能体系的最大挑战之一。对抗样本可能会导致机器学习模型的意外失败，或使模型容易受到网络攻击。除了加入一个噪声层，在图片中贴入面积稍小的黑白块、部分区域的马赛克也可能出现上述模型预测出错的情况。因此，**数据增强**、**大规模网络（主要是参数量大）**、**对抗网络**、**仿生单元**等增强网络鲁棒性的方法因运而生，但是这里暂不讨论。

* 那么能不能从频域学习的角度上来解释CNN的泛化性能？能不能利用频域学习降低数据冗余、增强模型鲁棒性？频域学习有什么样的优势？



## 二、发展历程

### 2.1 基于DCT图像数据的CNN应用研究

* 第一篇用CNN在频域（DCT系数）而不是空域（RGB）上直接学习的文章是《[On using CNN with DCT based Image Data](https://link.zhihu.com/?target=https%3A//www.scss.tcd.ie/Rozenn.Dahyot/pdf/IMVIP2017_MatejUlicny.pdf)》（这篇文章现在网上找不到了？！），这篇**文章在图像分类任务**上做的实验，包括MNIST、CIFAR10。

* 此后，在2018年 NIPS 会议上，《[Faster Neural Networks Straight from JPEG](https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/7649-faster-neural-networks-straight-from-jpeg.pdf)》这篇文章则进一步**用更大的模型（ResNet50）在更大的分类数据集（ImageNet分类任务）上做了更多的实验**。这篇文章直接提出了一种简单的想法：直接基于分块DCT量化压缩编码的`JPEG`图像训练CNN网络。

  * 为什么要这要做？

    * **JPEG图片存储到本地时，保存的是有损或者无损的DCT量化系数，而我们通常训练CNN都需要将图片解码为RGB格式，这个解码过程实际上比较耗时**。那么直接从本地图片中读取DCT系数，可以加快神经网络的执行效率，并降低数据传输带宽，如下图所示。

    <div align='center'>
        <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210323205934009.png" width=550>
    </div>

  * 这样做有什么样的优势？

    * 常规的`JPEG`解码为`RGB`图像的最后两步被跳过了，因此数据从CPU传输到GPU**数据量小了两倍**，并且图像数据已经在频域成分中
    *  一般地，在普通CNN中，浅层网络已经实现了对频域的转换，或者说是对频域信息的隐式建模，使模型能够学习到高频的信息。因此可以对于这种基于DCT频域学习的方法**可以使用较少层的神经网络**

  * 文中的网络是怎么设计的？

    * 在一般情况下啊，卷积神经网络对于`RGB`每一个通道的感受野都是同步的。但是经过`DCT`压缩变换后，`Y Cb Cr`中，`Cb`和`Cr`都只有`Y`分量的**一半**。因此本文中对`ResNet 50`的前几层进行了改造：

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210323235054856.png" width=900>
      </div>

    * 原始的`ResNet50`如图(a)所示。文中一共提出了7中改造的方法。如上图所示的两种方法是：**第一种**：把`Cb`、`Cr`的频率域分量上采样两倍，然后Concat到一起后，整体输入到修改后的ResNet50-Block2（因为原始的ResNet50-Block2的输入通道是64，但是这里DCT频域拼接后的通道数量是192）；**第二种**：直接对`Cb`、`Cr`的频率域分量进行卷积，即`Y`分量的频域成分经过ResNet50-Block3，输入通道数量是64，输出通道数量是512，而`Cr`、`Cb`分量的频域成分则通过一个新增的卷积单元，三个成分的卷积结果拼接到一起，然后整体输入到ResNet50-Block4

    * 当然这是文章中提出的几个改造方法中的其中两个，详见论文，这里就不一一赘述了，最后各项对比结果如下所示：

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324001757582.png" width=700>
      </div>

  * 有什么比较有趣的结论？

    * 除了利用DCT频域学习能够推理效率，提高精度之外，为了证明本文基于DCT频域学习的性能提升是由`DCT`频域变换这个**先验知识**得到的，而不是因为改变网络结构得到的，作者还做了消融实验：将基于`RGB`图像的第一层的卷积核设置为8，且步长也设置为8，用于模拟DCT的输出尺度和行为。结果表明：利用一个卷积核去模拟DCT实际上很困难，因为卷积核并不能直接学习到`DCT`完全正交的数据模式，并且得到的网络性能欠佳。 
    * 因此，直接利用DCT权重可提供更好网络训练性能，**因为DCT频域学习实际上直接提供了图像的 各频率成分 且 频率成分正交 的先验**，因此网络模型不需要过多的参数和冗余计算，来拟合这种频率成分的分布，自然而然也就提示了模型的效率。

* 这篇文章实际上还有很多的改进空间。例如高频冗余信息可以丢弃，可以扩展到图像分割领域等等。然后在2020年的CVPR上，阿里达摩院发表了一篇名为《[Learning in the Frequency Domain](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.12416.pdf)》的论文，将应用从图像分类拓广到了实例分割，另外包装得也更好。文章里提到**“learning-based dynamic channel selection method”**也是一个贡献，和通道裁剪的方法基本上比较类似。这篇文章的主题是：**旨在减少输入数据的大小，而不是模型的复杂性！。**

  * 为什么需要频域？（理由和上一篇论文一致）

    * 在传统方法中，通常在CPU上对高分辨率RGB图像进行预处理，然后将其传输到GPU / AI加速器以进行实时推断。 **由于RGB格式的未压缩图像通常较大，因此对CPU和GPU / AI加速器之间的通信带宽的要求通常很高**。为了降低计算成本和通信带宽需求，**将高分辨率RGB图像下采样为较小的图像，这通常会导致信息丢失和推理精度降低**。

  * 为什么可以做频域的通道裁剪？

    * 在《[Faster Neural Networks Straight from JPEG](https://link.zhihu.com/?target=https%3A//papers.nips.cc/paper/7649-faster-neural-networks-straight-from-jpeg.pdf)》一文中，作者只是去除了`DCT`解码为`RGB`图像的最后两步，但是实际上，并不是所有的高频成分都是对模型有作用的，换句话说，就是**存在输入数据冗余**。换个角度来看，如果从人眼的角度出发，人类视觉系统（HVS）总是对较为低频的信息比较敏感，而对高频信息不是那么敏感，图像压缩也是利用了这一点。

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324101916047.png" width=850>
      </div>

    * 此外，对输入的DCT频域通道继续裁剪，可以极大提高硬件内存带宽的利用率，降低嵌入式设备上计算资源和内存受限带来的各种限制：例如只能使用低分辨率的RGB图像、图像尺寸变小带来的信息丢失。

  * 如何做频域通道裁剪？

    * 本文中的频域通道裁剪采用的是**通道选择**的方案：实现这一功能的模块叫`dynamic gate module`，实际上就是SE-Net中提出的SE-Block，该模块为每个channel分配一个二进制分数。 分数为零的输入channel从网络中分离出来，然后提出一种称为**Gumbel Softmax**（如果不懂可以参见参考资料 - **博客文章[6]和[7]**）技巧的重新参数化方法，该方法允许梯度通过离散采样过程反向传播，如下图所示。

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324104630473.png" width=450>
      </div>

    * 这里为什么要用`Gumbel-Softmax`，而不是直接像`SE-Block`那样直接加入一个`Softmax`呢？这里`Gumbel-Softmax `实际上是强化学习，VAE中用到的**重参数化**的技巧。放到这里的目的主要是用于**在已有条件下，探索其他可能的频域通道选择方案**：举个例子，在某一轮训练过程中，经过**SE-Block**得到的`1x1xC`的特征向量，经过`Softmax`后，得到的 “概率输出” 实际上是固定的确定值，那么频域通道选择的方案也是固定的。那么假如`Gumbel`随机分布后，能够以一定概率扰动`Softmax`得到的概率分布，比如直接使用`Softmax`选择的通道是[1, 3, 4]，那么经过`Gumbel-Softmax`后选择的通道可能是[1, 3, 6]，增加了模型的探索度。而且`Gumbel-Softmax`整体上是`Softmax`的形式，在网络模型训练期间，梯度是可导的！

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324111824837.png" width=850>
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324112144899.png" width=850>
      </div>

  * 能够去除掉多少数据冗余？

    * 作者在ImageNet和CoCo数据集上做了测试，结果如下所示，每一个矩阵代表的是频域的`Y`、`Cb`和`Cr`通道，而矩阵上的每一个像素值代表的是：在这个数据集上，每一个频域通道被选择的平均概率，颜色越深代表选择的概率越大，如下图所示

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324114150946.png" width=700>
      </div>

  * 有什么有价值的结论？

    * 与高频（索引较大的框）相比，低频（索引较小的框）的选择频率更高。 这表明对于视觉推理任务而言，低频通道通常比高频通道更具信息性
    * 与色度分量Cb和Cr中的频道相比，模型更频繁地选择亮度分量Y中的频道。 这表明亮度分量对于视觉推理任务更具参考价值
    * 热图表明在分类和分割任务之间共享一个公共的频域通道选择模式。 这表明上述两个观察结果并非特定于一项任务，很可能对更高层次的视觉任务具有普遍性
    * 这些观察结果暗示CNN模型可能确实表现出与人类视觉类似的特征，并且针对人眼的图像压缩标准（例如JPEG）也可能适用于CNN模型。

  * 那么保留多少低频成分就可以达到比较好的效果了？

    * 在最后更为详细的对比实验中，作者也探究了这种频域通道选择的“精确形状”具体怎么选比较好：（1）直接使用`dynamic gate module`得到的结果作为选择方案，如`DCT-24D`表示**动态选择DCT的24个通道**（14Y+5Cb+5Cr）；（2）根据热图所示，选择左上角的“三角形”区域，如`DCT-24S`；（3）根据热图所示，选择左上角的“矩形”区域，如`DCT-24T`。以`ResNet50`和`MobileNetV2`作为主干网络，在`ImageNet`上对比实验如下图所示：

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324164049632.png" width=550>
      </div>

    * 在COCO数据集检测和分割任务，以`Mask R-CNN`作为测试网络的检测和分割结果如下所示：

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210324164628404.png" width=800>
      </div>

### 2.2 从频域的角度解释模型的泛化性能

* 在上一节中，我们用两篇文章简单叙述了下基于频域学习的发展和研究的应用场景。在《[Learning in the Frequency Domain](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2002.12416.pdf)》采用的频域通道选择的方法，在降低输入数据量，加快推理速度的情况下，实际上可以在一定程度上对抗**“高频噪声干扰”**，但是如果这个噪声换成了低频干扰，那么训练出的模型可能还是会出现“指鹿为马”的情况（比如：在不考虑数据增强情况下，贴上一些马赛克，或者低频扭曲什么的）。

* 那么，真的CNN就这么垃圾么？实际上也不全是。《[High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.pdf)》这篇`CVPR 2020`的Oral，从数据高低频分布出发探讨CNN泛化能力：其注意到CNN具备捕获人类无法感知的高频成分能力，而这个现象可以用于解释多种人类无法理解的假设，例如泛化能力、对抗样本鲁棒性等。
  * 发现了个什么问题？

    * 首先用CIFAR10在训练数据训练一个resnet18分类模型，接着在测试集上进行测试，此时可以得到模型正确率，接着进一步通过傅里叶变换，把原图转换到频域，再用一个半径阈值r=12，分离出高频部分和低频部分，再对高频和低频部分进行反傅里叶变换，得到高频重建图片和低频重建图片，然后测试这两张图片。结果出现了一些非常奇怪的现象：**模型对人眼看上去和原图差不多的低频图错误预测，反而正确预测了全黑的高频图**

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210325001411733.png" width=700>
      </div>

    * 这种图片在CIFAR10上大致有600张，虽然比例占整体的数据集不大，但是可以足够引起重视了

  * 那有什么样的假设呢？

    * **人类只能感知低频分量，而CNN对低频和高频分量都可以感知**，如图所示。

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210325002553722.png" width=700>
      </div>

    

    * 对于任何一个数据集，都应该包括语义信息(纹理信息或者说低频信息)和高频信息，只不过包括**比例不定而已，并且对于同一个分布数据集，其语义分布和高频分布都应该有自己的分布特性**。可以简单认为对于同一个类别标注的数据集，假设该数据集收集自多个场景，每个场景内的语义分布应该是近乎一致的(不然也不会标注为同一类)，但是高频分布就不一定了，可能和特定域有关，**该高频成分可能包括和类别相关的特定信息，也可以包括分布外的噪声，并且该噪声对模型训练是有害的，会影响泛化能力**。
    * 对于人类而言，标注时候由于无法感知高频成分故仅仅依靠语义进行标注，忽略了高频成分。但是CNN训练时候会同时面对语义低频成分和高频成分，这个gap就会导致CNN学习出来的模型和人类理解的模型不一样，从而出现常规的泛化认知错误。这一切实际上不能怪CNN，而是数据分布特性决定的。
    * 未避免被后续论文打脸，还特意指出：**本文并没有说模型有捕捉高频信号的倾向性，这里的主要观点是：模型并没有任何理由忽略高频信息，从而导致模型学到了高频和语义的混合信息**

  * 到底是什么引起了这种现象？

  

  * 问：“如果神经网络可以轻松地记住数据，为什么却不愿意从数据中学习可推广的模式，而不是直接记住一切以减少训练损失呢？"

    <div align="center">
        <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210325203640815.png" width=500>
    </div>

    * 在`Natural Label`的情况下，模型将首先拾取低频成分进行训练，然后逐渐拾取高频成分以实现更高的训练精度。
    * 在`Shuffled Label`的情况下，由于混洗会削弱低频成分和标签之间的关联性，因此模型在记住图像的时候会对低频和高频成分进行公平的对待

  * 问：“如果一个模型可以利用多个不同的信号集（高频和低频），那么为什么基于`Natural Label`的模型倾向于学习恰好与人类感知偏好相吻合的低频成分？”
    * 作者推测这仅仅是因为，由于数据集是由人组织和注释的，因此低频成分与标签的关联程度比高频成分与标签的关联程度更“有泛性”  。**因此拾取与标签关联性更强的低频成分可以让损失在训练的一开始就可以下降得很陡峭**。而到了训练的后期阶段，以`Natural Label`为标签的模型会依旧尝试着让模型朝着损失更小的方向进行训练，因此会逐步挖掘人眼感官并不敏感的高频信号来优化模型。
  * 启发式组建的对比
    * 大batch size训练时候,会同时考虑更多的非噪声的同分布高频成分，从而可以缩小训练和测试acc的误差间隔
    * dropout由于加入后和不加入时候差不多，所以看不出啥规律
    * 加入mixup后，训练acc和测试acc曲线gap变小，原因是mixup操作其实相当于混淆了低频语义信息，从而鼓励CNN尽可能多的去捕获高频信息
    * 当引入对抗样本后，cnn精度快速下降，原因是对抗样本可能是改变了高频分布(因为人眼无法感知),而训练过程中实际上学到的高频分布和对抗样本的高频分布不一致，从而CNN会完全预测错误
    * 对于BatchNorm就比较有意思了，作者拿出来做了单独的分析
    * **如果试图从数据的高低频分布以及CNN先学低频再学高频这个特性进行分析目前所提组件，是完全可以解释通的**
  * 作者假设BN优势之一是通过归一化来对齐不同预测信号的分布差异，没有经过BN训练的模型可能无法轻松获取这些高频成分，而且高频成分通常是较小的幅度，通过归一化后可以增加其幅值。总之BN层引入可以让模型轻易捕获高频成分，并且由于对齐效应，大部分捕获的会是有用高频成分，从而加快收敛速度，提高泛化能力
  * 当仅使用LFC训练模型时，无论是通过原始数据还是通过相应的LFC数据进行测试，BatchNorm都不总是有助于提高预测性能。 同样，半径越小，BatchNorm的帮助越少。 

* 作者认为：可能已经注意到，我们对高频成分的观察结果可以与“对抗性例子”现象直接相关联：即如果预测依赖于高频成分，那么高频成分的扰动将显着改变模型的响应，但是对于人类，这种扰动可能不会被观察到。 所有这些，创造了神经网络的非直觉行为。

  * 因此作者专门对模型对模型鲁棒性与模型利用高频成分的趋势做了一个实验：

    * 第一步讨论了卷积核“平滑度”与模型对高频成分的敏感性之间的联系
    * 我们首先显示出对抗性强的模型倾向于具有“平滑的”内核，然后证明直接平滑内核（无需训练）可以帮助提高对某些攻击的对抗性鲁棒性。

  * 如卷积定理所述，图像的卷积运算等效于图像频域的逐元素乘法。 因此，粗略地讲，如果卷积核在高频的权重可忽略不计，它将相应地权衡高频分量。这可能仅适用于第一层的卷积内核，因为较高层的内核并不直接与数据相关，因此这种关系尚不清楚。因此，我们认为，要推动模型忽略HFC，可以考虑强迫模型学习在频域高端仅权重可忽略的卷积核。

  *  根据信号处理知识，如果卷积核是“平滑的”，这意味着相邻权重之间没有剧烈的波动，则相应的频域将看到数量可忽略不计的高频信号。

  * 有什么结论？

    * 鲁棒的模型具有平滑的卷积核

      <div align="center">
          <img src="https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210326093852211.png" width=500>
      </div>



* 平滑内核可提高对抗性的鲁棒性
  * §6.1中的直观论证和§6.2中的经验发现直接引发了一个问题，即我们是否可以通过平滑第一层的卷积核来提高模型的对抗性鲁棒性。

![image-20210326095131514](https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210326095131514.png)

* 如表2所示，当应用我们的平滑方法时，清洁精度的性能直接下降，但对抗性鲁棒性的性能有所提高。 特别地，当允许摄动较大时，我们的方法会有所帮助。 例如，当= 0:09（大约23 = 255）时，Mnatural（ρ= 1：0）甚至胜过Madversarial。 通常，我们的方法可以轻松提高Mnatural的对抗鲁棒性，但只能在较大的情况下对Madversarial进行改进，这可能是因为Madversarial是使用PGD（= 0:03）作为威胁模型进行训练的

  ![image-20210326095503607](https://markdow-picbed.oss-cn-beijing.aliyuncs.com/img/image-20210326095503607.png)

  * 对抗的鲁棒模型趋向于让卷积核变得光滑，反过来不一定成立

### 2.3 把频域学习做到通道注意力

* 后续填坑

  

## 三、总结思考

* 基于DCT频域学习时，训练过程中存在的弊端： 在训练期间使用`JPEG`编码的频域学习的局限性在于进行数据扩充，例如 通过随机裁剪，在获取数据增强后的DCT系数之前，必须先对图像进行解压缩、数据增强，然后重新编码为`DCT`形式。实际上，只要涉及频域学习，数据数据增广的效率就大打折扣。
* JPEG量化压缩存在的缺点：`JPEG`压缩首先将图像分成**8×8像素块**，然后在每个块上进行离散余弦变换（DCT）。 然后每个块的DCT系数上都会应用量化压缩，且每一个DCT系数都具有已知的量化级别。 而且，压缩噪声比其他常见噪声类型更难建模，即：**与传统的假设噪声为白噪且与信号相互独立不同，量化操作的非线性使量化噪声不平稳且与图像信号本身相关**。
* 总结这几篇论文，基于频域学习的方法听起来比较高大上，实际上**也就是换了个数据的表征方式，然后喂给神经网络模型**。由于图像分解为高低频数据域后是相互正交的，而且可以相互分离，所以这样的研究更加适合于工业上的应用：例如，数据压缩、去除冗余以及模型裁剪达到提高执行效率、减少数据带宽的目的。目前也有很多的低功耗硬件编码硬件解码方案，关于图片的`DCT`频域压缩甚至不需要把他写到模型里面去，只需要把`JPEG`文件解码后读出使用即可
* 在《[High-frequency Component Helps Explain the Generalization of Convolutional Neural Networks](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_High-Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks_CVPR_2020_paper.pdf)》一文中提到的，**利用高频能够解释模型的泛化性能，但是在对高频成分做一些研究，来实现模型抵抗对抗样本的研究，却实在显得有些捉襟见肘**。第一个是因为实验中也做了试验，将卷积核变得更加平滑，虽然提高了模型的抵抗对抗样本的能力，但是模型的准确性也明显降低；第二个是因为对抗样本不仅仅只有高频攻击，遮挡、模糊、裁剪切割这些能够明显影响模型行为的方法，也可以称之为对抗样本。



## 参考资料DI

### (1) 博客文章

* [1] 频域（DCT,小波变换）与CNN结合 | [[知乎 - 简单控]](https://zhuanlan.zhihu.com/p/342991714)
* [2] CNN — 我不是你想的那样 | [[知乎 - 深度眸]](https://zhuanlan.zhihu.com/p/315601295)
* [3] 也谈阿里达摩院的频域学习论文 | [[知乎 - mileistone]](https://zhuanlan.zhihu.com/p/115584408)
* [4] 即插即用：把仿生模块和CNN拼接，对抗攻击鲁棒性显著提高！| [[腾讯云社区]](https://cloud.tencent.com/developer/article/1762810)
* [5] CMU团队解析CNN泛化能力：一切秘密都在数据中 | [[知乎 - 机器之心]](https://zhuanlan.zhihu.com/p/248068207?from=timeline)
* [6] 【一文学会】Gumbel-Softmax的采样技巧 | [[CSDN - 谁把小明注册了]](https://blog.csdn.net/weixin_40255337/article/details/83303702)
* [7] Re-parameterization，Gumbel-Softmax | [[简书 - VanJordan]](https://www.jianshu.com/p/9d5a0698f982)
* [8] CMU团队解析CNN泛化能力：一切秘密都在数据中 | [[知乎 - 机器之心]](https://zhuanlan.zhihu.com/p/248068207?from=timeline)

### (2) 相关论文

* [1] Ulicny, Matej, and Rozenn Dahyot. "**On using cnn with dct based image data.**" *Proceedings of the 19th Irish Machine Vision and Image Processing conference IMVIP*. Vol. 2. 2017.
* [2] Gueguen, Lionel, et al. "**Faster neural networks straight from jpeg.**" *Advances in Neural Information Processing Systems* 31 (2018): 3933-3944.
* [4] Rajesh, B. et al. “**DCT-CompCNN: A Novel Image Classification Network Using JPEG Compressed DCT Coefficients.**” *2019 IEEE Conference on Information and Communication Technology* (2019): 1-6.
* [5] Wang, Haohan, et al. "**High-frequency component helps explain the generalization of convolutional neural networks.**" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. | [[CMU]](https://arxiv.org/abs/1905.13545)[[Github]](https://github.com/HaohanWang/HFC)
* [6] Xu, Kai, et al. "**Learning in the frequency domain.**" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2020. | [[阿里达摩院]](https://arxiv.org/pdf/2002.12416.pdf)[[Github]](https://github.com/calmevtime/DCTNet)
* [7] Qin, Zequn, et al. "**FcaNet: Frequency Channel Attention Networks.**" *arXiv preprint arXiv:2012.11879* (2020). | [[浙大 - 李玺团队]](https://arxiv.org/abs/2012.11879)[[Github]](https://github.com/dcdcvgroup/FcaNet)

* [8] Shen, Xing, et al. "**DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation.**" *arXiv preprint arXiv:2011.09876* (2020). | [[阿里达摩院]](https://arxiv.org/pdf/2011.09876.pdf)

  

